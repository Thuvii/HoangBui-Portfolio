<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 4 - Customer Segmentation using Clustering</title>
    <style>
        body { font-family: Times New Roman, serif; font-size: 12pt; line-height: 1.5; margin: 40px 200px; color: #000; background: #f9f4ef; }
        h1 { font-size: 20pt; text-align: center; margin-bottom: 10px; }
        h2 { font-size: 14pt; margin-top: 25px; margin-bottom: 10px; }
        section { margin-bottom: 20px; }
        ul, ol { margin-left: 30px; }
        a { color: #4b3832; text-decoration: none; }
        table, th, td {
            border: 1px solid black;
            border-collapse: collapse;
            padding: 8px;
        }
        img { display: block; margin: 10px 0; max-width: 100%; }
        pre { background: #f4f4f4; padding: 10px; overflow-x: auto; }
    </style>
</head>
<body>
    <h1>Project 4: Customer Segmentation using Clustering</h1>
    <section>
        <h2>1. Introduce the Problem</h2>
        <p>
            In this project, the goal is to <strong>segment customers</strong> based on their purchasing behavior using clustering techniques.
            Businesses often have thousands of customers, but not all customers behave the same way.
            Some buy frequently, others rarely, and some spend much more money than others.
            Understanding these different groups (or <em>segments</em>) can help a business create <strong>targeted marketing strategies</strong>,
            improve customer retention, and increase overall revenue.
        </p>
        <p>The main questions I aim to answer are:</p>
        <ul>
            <li>Can we identify distinct customer groups based on their <strong>recency</strong>, <strong>frequency</strong>, and <strong>monetary value</strong>?</li>
            <li>What characterizes each group — for example, which cluster represents “loyal” or “high-value” customers?</li>
            <li>How can these insights help a business make data-driven marketing decisions?</li>
        </ul>
    </section>

    <section>
        <h2>2. What is Clustering and How Does it Work?</h2>
        <p>
            <strong>Clustering</strong> is an <strong>unsupervised machine learning</strong> technique that automatically groups similar data points together based on their features.
            Unlike classification, clustering does not use labeled data — it simply finds patterns in the data itself.
        </p>
       <p>In this project, I used two clustering algorithms: <strong>K-Means</strong> and <strong>Agglomerative Clustering</strong>.</p>

        <h4>1. K-Means Clustering</h4>
        <p>
        K-Means is one of the most common and efficient clustering algorithms. It works by dividing the data into <em>k</em> groups based on feature similarity.
        </p>
        <ol>
            <li>The algorithm randomly selects <em>k</em> cluster centers (called <strong>centroids</strong>).</li>
            <li>Each data point is assigned to the nearest centroid using a distance measure (usually <strong>Euclidean distance</strong>).</li>
            <li>The centroids are then updated by calculating the mean of all data points assigned to each cluster.</li>
            <li>Steps 2 and 3 repeat until the centroids stop changing significantly, meaning the model has converged.</li>
        </ol>

        <h4>2. Agglomerative (Hierarchical) Clustering</h4>
        <p>
        Agglomerative Clustering is a type of hierarchical clustering that builds clusters step by step, starting from individual points and merging them into larger groups.
        </p>
        <ol>
            <li>Initially, each data point is treated as its own cluster.</li>
            <li>The algorithm finds the two most similar clusters (based on a chosen <strong>linkage method</strong> such as Ward, Complete, or Average linkage).</li>
            <li>Those two clusters are merged into one.</li>
            <li>This process repeats until all points belong to a single large cluster or until a stopping criterion (like a set number of clusters) is reached.</li>
        </ol>
        <p>
        The result can be visualized as a <strong>dendrogram</strong>, which shows the hierarchical relationships between clusters and helps identify the optimal number of clusters by where to “cut” the tree.
        </p>

    </section>

    <section>
        <h2>3. Introduce the Data</h2>
        <p>
            The dataset used is the
            <a href="https://www.kaggle.com/datasets/lakshmi25npathi/online-retail-dataset" target="_blank">
                Online Retail Dataset
            </a>
            from Kaggle. It contains transactions from a UK-based online retail store between 2009 and 2011.
        </p>
        <ul>
            <li><strong>InvoiceNo</strong>: invoice number</li>
            <li><strong>StockCode</strong>: product ID</li>
            <li><strong>Description</strong>: product name</li>
            <li><strong>Quantity</strong>: number of items purchased</li>
            <li><strong>InvoiceDate</strong>: date and time of the transaction</li>
            <li><strong>UnitPrice</strong>: price per item</li>
            <li><strong>CustomerID</strong>: unique identifier for each customer</li>
            <li><strong>Country</strong>: location of the customer</li>
        </ul>
        <p>For clustering, I used three key features derived from RFM analysis:</p>
        <ul>
            <li><strong>Recency:</strong> Days since the customer’s last purchase</li>
            <li><strong>Frequency:</strong> Number of unique purchases</li>
            <li><strong>Monetary:</strong> Total amount spent</li>
        </ul>
    </section>

    <section>
        <h2>4. Data Understanding / Visualization</h2>
        <p>
            To better understand the data, I computed <strong>RFM values</strong> for each customer and visualized their distributions.
        </p>
        <p>Before log-transform</p>
        <img src="../image/image_project4/before.png" alt="Top Countries">
        <p></p>
        <p>Top Countries With The Most Transactions</p>
        <img src="../image/image_project4/topCountries.png" alt="Top Countries">
        <p>Most transactions are from the UK, so we’ll focus on UK customers for consistent segmentation.</p>
    </section>

    <section>
        <h2>5. Pre-processing the Data</h2>
        <ol>
            <li>Removed missing values and invalid quantities.</li>
            <li>Aggregated by <strong>CustomerID</strong> to compute RFM metrics.</li>
            <li>Applied <strong>log transformation</strong> to reduce skewness.</li>
            <li>Standardized all features using <code>StandardScaler</code>.</li>
            <li>Verified there were no <code>NaN</code> values before modeling.</li>
        </ol>
        Histograms showed that <strong>Recency</strong> and <strong>Monetary</strong> were right-skewed, so a log transformation was applied to normalize the data.
        Scatter plots between <strong>Recency</strong>, <strong>Frequency</strong>, and <strong>Monetary</strong> revealed clear separation patterns, for example, customers with high frequency tended to have low recency (recent purchases), while those with low frequency often had high recency and low spending.
        This pattern confirmed that RFM features were meaningful for clustering since they naturally group customers by purchasing behavior and value.
        <p>After log-transform</p>
        <img src="../image/image_project4/after.png" alt="Top Countries">
        <p>Scatter plots</p>
        <img src="../image/image_project4/pair.png" alt="">
    </section>

    <section>
        <h2>6. Modeling (Clustering)</h2>
        <h4>1. K-Means Clustering</h4>
        <p>Elbow</p>
        <img src="../image/image_project4/elbow.png" alt="">
        <p>The plot shows the "elbow" point where the decrease in Inertia (WCSS) starts to flatten out significantly is at k=4 (or k=3). This indicates that 4 clusters efficiently minimizes the overall variance within the groups, justifying the cluster split.</p>
        <p>Silhouette</p>
        <img src="../image/image_project4/silhouette.png" alt="">
        <p>The Silhouette Score at k=4 (=0.34) is the second-highest score after k=2 (=0.44). Although k=2 has the best separation, the score at k=4 is still reasonably good, suggesting the 4 clusters are fairly well-separated from each other</p>
        <p>Both K-Means and Agglomerative clustering were applied, with the elbow and silhouette methods suggesting k=4 as optimal.
            K-Means was selected as the primary model because it scales efficiently to larger datasets and works well with continuous numeric variables like log-transformed RFM features.
            </p>
        <p>Cluster Summary(k=4)</p>        
        <table>
            <tr><th>Cluster</th><th>Recency</th><th>Frequency</th><th>Monetary</th></tr>
            <tr><td>0</td><td>5.01</td><td>0.86</td><td>5.51</td></tr>
            <tr><td>1</td><td>2.07</td><td>2.65</td><td>8.27</td></tr>
            <tr><td>2</td><td>2.68</td><td>1.16</td><td>6.08</td></tr>
            <tr><td>3</td><td>4.01</td><td>1.72</td><td>7.16</td></tr>
        </table>
        <h4>2. Agglomerative (Hierarchical) Clustering</h4>
        <p>Hierarchical clustering dendrogram</p>
        <img src="../image/image_project4/deno.png" alt="">
        <p>A horizontal cut across the Dendrogram at a moderate distance (e.g., around 25) clearly intersects four major distinct branches.</p>
        <p>The hierarchical dendrogram supported the result k=4, showing clear group separation consistent with the K-Means.</p>
    </section>

    <section>
        <h2>7. Storytelling (Clustering Analysis)</h2>
        <p>The clusters describe four main customer groups:</p>
        <img src="../image/image_project4/image.png" alt="">
        <table>
            <tr><th>Cluster</th><th>Behavior Description</th><th>Interpretation</th></tr>
            <tr><td>0</td><td>High R, Low F, Medium M</td><td>Customers who haven’t purchased recently, buy infrequently, and spend moderately => "At Risk"</td></tr>
            <tr><td>1</td><td>Low R, High F, Very High M</td><td>Recent, frequent, and high-spending customers => Best / Loyal customers</td></tr>
            <tr><td>2</td><td>Medium R, Low F, Medium M</td><td>Customers who buy occasionally, moderate spenders => Potential / Regulars</td></tr>
            <tr><td>3</td><td>Medium-High R, Low F, Medium-High M</td><td>Haven’t bought recently but used to spend a lot => Lost Big Spenders</td></tr>
        </table>
        <p>
            Businesses can use these insights to improve marketing:<br>
            - marketing on Cluster 1, offer loyalty programs or VIP benefits.<br>
            - Cluster 3 with targeted win-back promotions.<br>
            - Try to convert Cluster 2 into Cluster 1 by offering consistent-value deals.<br>
            - Cluster 0, they might churn soon.<br>
        </p>
    </section>

    <section>
        <h2>8. Positive impact</h2>
        <ul>
            <li><strong>Enables targeted marketing</strong> — improves retention &amp; revenue.</li>
            <li><strong>Reduces blanket promotions</strong> — saves cost.</li>
            <li><strong>Guides inventory &amp; customer service focus</strong>.</li>
        </ul>

        <p>Possible negative impact</p>
        <ul>
            <li><strong>Over-targeting</strong> — may cause privacy or fairness concerns.</li>
            <li><strong>Poor data quality</strong> — could lead to unfair classifications.</li>
            <li><strong>Neglect of low-value customers</strong> — businesses might unfairly deprioritize them.</li>
        </ul>
    </section>


    <section>
        <h2>9. References</h2>
        <ul>
           <li><a href="https://www.kaggle.com/datasets/abhishekrp1517/online-retail-transactions-dataset" target="_blank">Kaggle Dataset: Online Retail Dataset</a></li>
        </ul>
    </section>

    <section>
        <h2>10. Code</h2>
        <p>
            <a href="https://github.com/Thuvii/dataMining/blob/main/project4/project4.ipynb">GitHub</a>
        </p>
    </section>
</body>
</html>
